{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install --quiet transformers torchviz\n",
        "! pip install --quiet accelerate sentencepiece datasets evaluate bitsandbytes tqdm\n",
        "!pip install --quiet pytorch-lightning # pytorch wrapper\n",
        "!pip install --quiet torchtext # text utilities"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yj1YhQL698ZE",
        "outputId": "f993d905-1c83-4975-9b92-210c4458190e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for torchviz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.3/519.3 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m725.0/725.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.6/731.6 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchviz import make_dot\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "h1hsAUu74Pky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATHzPkhz9uY2",
        "outputId": "5fc0aa0c-9708-4928-e624-c11645d031bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1369: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summarization:\n",
            "ServiceNow, Inc. is a company that provides IT management software for enterprise operations.\n",
            "\n",
            "Question Answering:\n",
            "IT management software\n",
            "\n",
            "Translation:\n",
            "Hello, c'est-ce-t-t-t-t-t-\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-small\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-small\")\n",
        "\n",
        "# Task 2: Summarization\n",
        "summarization_input = \"Summarize the following text: ServiceNow, Inc. provides enterprise information technology (IT) management software. The Company designs, develops, and markets a cloud computing platform to help companies manage digital workflows for enterprise operations.\"\n",
        "input_ids_summarization = tokenizer(\"summarize: \" + summarization_input, return_tensors=\"pt\").input_ids\n",
        "summary_ids = model.generate(input_ids_summarization)\n",
        "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "print(\"Summarization:\")\n",
        "print(summary)\n",
        "\n",
        "# Task 3: Question Answering\n",
        "context = \"ServiceNow, Inc. provides enterprise information technology (IT) management software. The Company designs, develops, and markets a cloud computing platform to help companies manage digital workflows for enterprise operations.\"\n",
        "question = \"What is ServiceNow known for?\"\n",
        "input_text_qa = f\"question: {question} context: {context}\"\n",
        "input_ids_qa = tokenizer(input_text_qa, return_tensors=\"pt\").input_ids\n",
        "answer_ids = model.generate(input_ids_qa)\n",
        "answer = tokenizer.decode(answer_ids[0], skip_special_tokens=True)\n",
        "print(\"\\nQuestion Answering:\")\n",
        "print(answer)\n",
        "\n",
        "# Task 4: English to French Translation\n",
        "translation_input = \"Translate the following English text to French: Hello, how are you?\"\n",
        "input_ids_translation = tokenizer(\"translate English to French: \" + translation_input, return_tensors=\"pt\").input_ids\n",
        "translation_ids = model.generate(input_ids_translation)\n",
        "translation = tokenizer.decode(translation_ids[0], skip_special_tokens=True)\n",
        "print(\"\\nTranslation:\")\n",
        "print(translation)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSfCyV2Z4odT",
        "outputId": "cd4ee206-5615-483c-b38c-f18536718788"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "T5ForConditionalGeneration(\n",
            "  (shared): Embedding(32128, 512)\n",
            "  (encoder): T5Stack(\n",
            "    (embed_tokens): Embedding(32128, 512)\n",
            "    (block): ModuleList(\n",
            "      (0): T5Block(\n",
            "        (layer): ModuleList(\n",
            "          (0): T5LayerSelfAttention(\n",
            "            (SelfAttention): T5Attention(\n",
            "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
            "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
            "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
            "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
            "              (relative_attention_bias): Embedding(32, 6)\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (1): T5LayerFF(\n",
            "            (DenseReluDense): T5DenseGatedActDense(\n",
            "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
            "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
            "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (act): NewGELUActivation()\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (1-7): 7 x T5Block(\n",
            "        (layer): ModuleList(\n",
            "          (0): T5LayerSelfAttention(\n",
            "            (SelfAttention): T5Attention(\n",
            "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
            "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
            "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
            "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (1): T5LayerFF(\n",
            "            (DenseReluDense): T5DenseGatedActDense(\n",
            "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
            "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
            "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (act): NewGELUActivation()\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (final_layer_norm): T5LayerNorm()\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (decoder): T5Stack(\n",
            "    (embed_tokens): Embedding(32128, 512)\n",
            "    (block): ModuleList(\n",
            "      (0): T5Block(\n",
            "        (layer): ModuleList(\n",
            "          (0): T5LayerSelfAttention(\n",
            "            (SelfAttention): T5Attention(\n",
            "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
            "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
            "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
            "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
            "              (relative_attention_bias): Embedding(32, 6)\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (1): T5LayerCrossAttention(\n",
            "            (EncDecAttention): T5Attention(\n",
            "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
            "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
            "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
            "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (2): T5LayerFF(\n",
            "            (DenseReluDense): T5DenseGatedActDense(\n",
            "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
            "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
            "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (act): NewGELUActivation()\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (1-7): 7 x T5Block(\n",
            "        (layer): ModuleList(\n",
            "          (0): T5LayerSelfAttention(\n",
            "            (SelfAttention): T5Attention(\n",
            "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
            "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
            "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
            "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (1): T5LayerCrossAttention(\n",
            "            (EncDecAttention): T5Attention(\n",
            "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
            "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
            "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
            "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (2): T5LayerFF(\n",
            "            (DenseReluDense): T5DenseGatedActDense(\n",
            "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
            "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
            "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (act): NewGELUActivation()\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (final_layer_norm): T5LayerNorm()\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nModel Layers:\")\n",
        "for i, layer in enumerate(model.encoder.block):\n",
        "    print(f\"Block {i}: {layer}\")\n",
        "\n",
        "\n",
        "# Task 6: Print total number of parameters/weights in the model\n",
        "total_parameters = sum(p.numel() for p in model.parameters())\n",
        "print(\"\\nTotal Parameters:\", total_parameters)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-xc0we29zto",
        "outputId": "be9aa9c1-27c6-4194-e4bf-10c0e667b47c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model Layers:\n",
            "Block 0: T5Block(\n",
            "  (layer): ModuleList(\n",
            "    (0): T5LayerSelfAttention(\n",
            "      (SelfAttention): T5Attention(\n",
            "        (q): Linear(in_features=512, out_features=384, bias=False)\n",
            "        (k): Linear(in_features=512, out_features=384, bias=False)\n",
            "        (v): Linear(in_features=512, out_features=384, bias=False)\n",
            "        (o): Linear(in_features=384, out_features=512, bias=False)\n",
            "        (relative_attention_bias): Embedding(32, 6)\n",
            "      )\n",
            "      (layer_norm): T5LayerNorm()\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (1): T5LayerFF(\n",
            "      (DenseReluDense): T5DenseGatedActDense(\n",
            "        (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
            "        (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
            "        (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (act): NewGELUActivation()\n",
            "      )\n",
            "      (layer_norm): T5LayerNorm()\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Block 1: T5Block(\n",
            "  (layer): ModuleList(\n",
            "    (0): T5LayerSelfAttention(\n",
            "      (SelfAttention): T5Attention(\n",
            "        (q): Linear(in_features=512, out_features=384, bias=False)\n",
            "        (k): Linear(in_features=512, out_features=384, bias=False)\n",
            "        (v): Linear(in_features=512, out_features=384, bias=False)\n",
            "        (o): Linear(in_features=384, out_features=512, bias=False)\n",
            "      )\n",
            "      (layer_norm): T5LayerNorm()\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (1): T5LayerFF(\n",
            "      (DenseReluDense): T5DenseGatedActDense(\n",
            "        (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
            "        (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
            "        (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (act): NewGELUActivation()\n",
            "      )\n",
            "      (layer_norm): T5LayerNorm()\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Block 2: T5Block(\n",
            "  (layer): ModuleList(\n",
            "    (0): T5LayerSelfAttention(\n",
            "      (SelfAttention): T5Attention(\n",
            "        (q): Linear(in_features=512, out_features=384, bias=False)\n",
            "        (k): Linear(in_features=512, out_features=384, bias=False)\n",
            "        (v): Linear(in_features=512, out_features=384, bias=False)\n",
            "        (o): Linear(in_features=384, out_features=512, bias=False)\n",
            "      )\n",
            "      (layer_norm): T5LayerNorm()\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (1): T5LayerFF(\n",
            "      (DenseReluDense): T5DenseGatedActDense(\n",
            "        (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
            "        (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
            "        (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (act): NewGELUActivation()\n",
            "      )\n",
            "      (layer_norm): T5LayerNorm()\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Block 3: T5Block(\n",
            "  (layer): ModuleList(\n",
            "    (0): T5LayerSelfAttention(\n",
            "      (SelfAttention): T5Attention(\n",
            "        (q): Linear(in_features=512, out_features=384, bias=False)\n",
            "        (k): Linear(in_features=512, out_features=384, bias=False)\n",
            "        (v): Linear(in_features=512, out_features=384, bias=False)\n",
            "        (o): Linear(in_features=384, out_features=512, bias=False)\n",
            "      )\n",
            "      (layer_norm): T5LayerNorm()\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (1): T5LayerFF(\n",
            "      (DenseReluDense): T5DenseGatedActDense(\n",
            "        (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
            "        (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
            "        (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (act): NewGELUActivation()\n",
            "      )\n",
            "      (layer_norm): T5LayerNorm()\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Block 4: T5Block(\n",
            "  (layer): ModuleList(\n",
            "    (0): T5LayerSelfAttention(\n",
            "      (SelfAttention): T5Attention(\n",
            "        (q): Linear(in_features=512, out_features=384, bias=False)\n",
            "        (k): Linear(in_features=512, out_features=384, bias=False)\n",
            "        (v): Linear(in_features=512, out_features=384, bias=False)\n",
            "        (o): Linear(in_features=384, out_features=512, bias=False)\n",
            "      )\n",
            "      (layer_norm): T5LayerNorm()\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (1): T5LayerFF(\n",
            "      (DenseReluDense): T5DenseGatedActDense(\n",
            "        (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
            "        (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
            "        (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (act): NewGELUActivation()\n",
            "      )\n",
            "      (layer_norm): T5LayerNorm()\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Block 5: T5Block(\n",
            "  (layer): ModuleList(\n",
            "    (0): T5LayerSelfAttention(\n",
            "      (SelfAttention): T5Attention(\n",
            "        (q): Linear(in_features=512, out_features=384, bias=False)\n",
            "        (k): Linear(in_features=512, out_features=384, bias=False)\n",
            "        (v): Linear(in_features=512, out_features=384, bias=False)\n",
            "        (o): Linear(in_features=384, out_features=512, bias=False)\n",
            "      )\n",
            "      (layer_norm): T5LayerNorm()\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (1): T5LayerFF(\n",
            "      (DenseReluDense): T5DenseGatedActDense(\n",
            "        (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
            "        (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
            "        (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (act): NewGELUActivation()\n",
            "      )\n",
            "      (layer_norm): T5LayerNorm()\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Block 6: T5Block(\n",
            "  (layer): ModuleList(\n",
            "    (0): T5LayerSelfAttention(\n",
            "      (SelfAttention): T5Attention(\n",
            "        (q): Linear(in_features=512, out_features=384, bias=False)\n",
            "        (k): Linear(in_features=512, out_features=384, bias=False)\n",
            "        (v): Linear(in_features=512, out_features=384, bias=False)\n",
            "        (o): Linear(in_features=384, out_features=512, bias=False)\n",
            "      )\n",
            "      (layer_norm): T5LayerNorm()\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (1): T5LayerFF(\n",
            "      (DenseReluDense): T5DenseGatedActDense(\n",
            "        (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
            "        (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
            "        (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (act): NewGELUActivation()\n",
            "      )\n",
            "      (layer_norm): T5LayerNorm()\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Block 7: T5Block(\n",
            "  (layer): ModuleList(\n",
            "    (0): T5LayerSelfAttention(\n",
            "      (SelfAttention): T5Attention(\n",
            "        (q): Linear(in_features=512, out_features=384, bias=False)\n",
            "        (k): Linear(in_features=512, out_features=384, bias=False)\n",
            "        (v): Linear(in_features=512, out_features=384, bias=False)\n",
            "        (o): Linear(in_features=384, out_features=512, bias=False)\n",
            "      )\n",
            "      (layer_norm): T5LayerNorm()\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (1): T5LayerFF(\n",
            "      (DenseReluDense): T5DenseGatedActDense(\n",
            "        (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
            "        (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
            "        (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (act): NewGELUActivation()\n",
            "      )\n",
            "      (layer_norm): T5LayerNorm()\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "\n",
            "Total Parameters: 76961152\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set the tensor in final layer (decoder.final_layer_norm.weight) to all zeros."
      ],
      "metadata": {
        "id": "JafuW_KZ5JNh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Set the weights of the final layer's normalization to zeros\n",
        "model.decoder.final_layer_norm.weight.data.fill_(0.0)\n",
        "\n",
        "# Verify if the Q&A task works after resetting the weights\n",
        "question = \"What is ServiceNow?\"\n",
        "input_text_qa = f\"question: {question} context: {context}\"\n",
        "input_ids_qa = tokenizer(input_text_qa, return_tensors=\"pt\").input_ids\n",
        "answer_ids = model.generate(input_ids_qa)\n",
        "print(input_text_qa)\n",
        "print(input_ids_qa)\n",
        "answer = tokenizer.decode(answer_ids[0], skip_special_tokens=True)\n",
        "print(\"\\nQuestion Answering After Resetting Weights:\")\n",
        "print(answer)\n",
        "\n",
        "## The question answering does not works after resetting the weights\n",
        "## setting everything to zero is responsible for loss of expressiveness and the output of a linear layer with all zero weights becomes a constant value, often zero. This happens because the multiplication of all input features by zero results in an output of zero, regardless of the input values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yv4X-xmP_Dd7",
        "outputId": "0a34e477-9e23-4a0c-f3c0-f378b9623aa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "question: What is ServiceNow? context: ServiceNow, Inc. provides enterprise information technology (IT) management software. The Company designs, develops, and markets a cloud computing platform to help companies manage digital workflows for enterprise operations.\n",
            "tensor([[  822,    10,   363,    19,  1387, 17527,    58,  2625,    10,  1387,\n",
            "         17527,     6,  1542,     5,   795,  5399,   251,   748,    41,  3177,\n",
            "            61,   758,   889,     5,    37,  1958,  2888,     6,  1344,     7,\n",
            "             6,    11,  3212,     3,     9,  3126, 10937,  1585,    12,   199,\n",
            "           688,  1865,  1125, 16101,     7,    21,  5399,  2673,     5,     1]])\n",
            "\n",
            "Question Answering After Resetting Weights:\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# Define the new dimensions for the smaller layer\n",
        "new_dim = 512  # Adjust this value as needed\n",
        "\n",
        "# Replace the final layer normalization with a smaller layer\n",
        "new_final_layer_norm = nn.LayerNorm(new_dim)\n",
        "model.decoder.final_layer_norm = new_final_layer_norm\n",
        "\n",
        "# Adjust other dependent layers to match the new dimension\n",
        "model.decoder.block[0].linear1 = nn.Linear(new_dim, model.config.d_model)\n",
        "model.decoder.block[0].linear2 = nn.Linear(model.config.d_model, new_dim)\n",
        "model.decoder.block[1].linear1 = nn.Linear(new_dim, model.config.d_model)\n",
        "model.decoder.block[1].linear2 = nn.Linear(model.config.d_model, new_dim)\n",
        "\n",
        "# Verify if the Q&A task works after modifying the model\n",
        "question = \"What is Hugging Face known for?\"\n",
        "context = \"Hugging Face is known for its contributions to NLP research and its transformer-based models.\"\n",
        "input_text_qa = f\"question: {question} context: {context}\"\n",
        "input_ids_qa = tokenizer(input_text_qa, return_tensors=\"pt\").input_ids\n",
        "\n",
        "print(input_ids_qa)\n",
        "# Adjust input dimension to 256\n",
        "input_ids_qa = input_ids_qa[:, :256]\n",
        "\n",
        "answer_ids = model.generate(input_ids_qa)\n",
        "answer = tokenizer.decode(answer_ids[0], skip_special_tokens=True)\n",
        "print(\"\\nQuestion Answering After Modifying the Model:\")\n",
        "print(answer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ssN2RSE_lgT",
        "outputId": "f24660b8-ee27-42c3-c2d4-0bff72f242da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  822,    10,   363,    19, 11560,  3896,  8881,   801,    21,    58,\n",
            "          2625,    10, 11560,  3896,  8881,    19,   801,    21,   165,  7548,\n",
            "            12,   445,  6892,   585,    11,   165, 19903,    18,   390,  2250,\n",
            "             5,     1]])\n",
            "\n",
            "Question Answering After Modifying the Model:\n",
            "transformer-based models\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AqQirHB4ztEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3SUzHfcLQUSQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}