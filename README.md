# Fine Tuning flan-t5-small

Model for a Q&A task that takes a context as additional input along with the question.

Further works:

- Efficient Finetuning using LORA on a proper GPU

Note: The result of the output is not great because colab was running out of ram for this.
